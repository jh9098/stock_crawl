# .github/workflows/schedule_pipeline.yml
# ─────────────────────────────────────────────
name: Run Daily News Pipeline

on:
  # 1) GitHub Actions 탭에서 수동 실행
  workflow_dispatch:

  # 2) 매일 22:00 UTC(한국 07:00)에 자동 실행
  schedule:
    - cron: '0 22 * * *'

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    steps:
      # 1) 저장소 체크아웃
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2) Python 3.10 설치
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3) 의존성 설치
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests beautifulsoup4 lxml google-generativeai

      # 4) 파이프라인 실행
      - name: Run Python Pipeline
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          NAVER_CLIENT_ID: ${{ secrets.NAVER_CLIENT_ID }}
          NAVER_CLIENT_SECRET: ${{ secrets.NAVER_CLIENT_SECRET }}
        run: |
          python backend/run_pipeline.py

      # 5) 결과 CSV 업로드 ── 경로 수정됨!
      - name: Upload Artifact
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-stock-data
          # run_pipeline.py가 생성하는 실제 위치와 일치
          path: output/aggregated/aggregated_stock_data.csv
          retention-days: 5
