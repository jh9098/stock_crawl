# .github/workflows/schedule_pipeline.yml
---
# 워크플로우의 이름
name: Run Daily News Pipeline

# 이 워크플로우가 언제 실행될지를 정의
'on':
  # 1. GitHub Actions 탭에서 수동으로 실행할 수 있도록 설정
  workflow_dispatch:
  # 2. 스케줄에 따라 자동 실행 설정
  schedule:
    # 매일 22:00 UTC (한국 시간 오전 7시)에 실행
    # (매일 아침 새로운 뉴스를 분석)
    - cron: '0 22 * * *'

# 실행될 작업(Job)들을 정의
jobs:
  # 'run-pipeline' 이라는 이름의 단일 작업
  run-pipeline:
    # 이 작업이 실행될 가상 환경을 지정 (최신 우분투)
    runs-on: ubuntu-latest

    # 작업의 단계(Step)들을 순서대로 정의
    steps:
      # 1단계: GitHub 저장소의 코드를 가상 환경으로 가져옴 (Checkout)
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2단계: 파이썬 특정 버전(3.10)을 설치
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3단계: 파이썬 스크립트 실행에 필요한 라이브러리들을 설치
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests beautifulsoup4 lxml google-generativeai

      # 4단계: 메인 파이썬 파이프라인 스크립트를 실행
      - name: Run Python Pipeline
        env:
          # GitHub Secrets에 저장된 값들을 스크립트 내에서 사용할 수 있도록 환경 변수로 주입
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          NAVER_CLIENT_ID: ${{ secrets.NAVER_CLIENT_ID }}
          NAVER_CLIENT_SECRET: ${{ secrets.NAVER_CLIENT_SECRET }}
        run: |
          # backend 폴더 안에 있는 run_pipeline.py를 실행
          python backend/run_pipeline.py

      # 5단계: 파이프라인 실행 후 생성된 결과물(CSV 파일)을 업로드
      - name: Upload Artifact
        # 공식 'upload-artifact' 액션을 사용
        uses: actions/upload-artifact@v4
        with:
          # 업로드될 아티팩트의 이름
          name: aggregated-stock-data
          # 업로드할 파일의 경로
          # run_pipeline.py 스크립트가 생성하는 CSV 파일의 위치와 일치해야 함
          # (run_pipeline.py writes to the repository root's output
          #  aggregated directory)
          path: output/aggregated/aggregated_stock_data.csv
          # 결과물을 보관할 기간 (일 단위)
          # 너무 길게 설정하면 저장 공간을 많이 차지하므로 적절히 조절
          retention-days: 5
