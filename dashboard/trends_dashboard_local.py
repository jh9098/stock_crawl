# dashboard/trends_dashboard.py
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import plotly.express as px
import os
import csv
import numpy as np
import ast
from pathlib import Path
from math import log
import requests # requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í™•ì¸

# =========================== ê¸°ë³¸ ì„¤ì • ===========================
st.set_page_config(
    page_title="ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“ˆ",
    layout="wide",
)

# --- [í•µì‹¬ ìˆ˜ì •] GitHub Raw URLì—ì„œ ë°ì´í„° ë¡œë“œ ---
# ğŸš¨ ì•„ë˜ URLì˜ 'YourUsername/YourRepoName' ë¶€ë¶„ì„ 
#    ë³¸ì¸ì˜ ì‹¤ì œ GitHub ì‚¬ìš©ìëª…ê³¼ ì €ì¥ì†Œ ì´ë¦„ìœ¼ë¡œ ë°˜ë“œì‹œ ë°”ê¿”ì£¼ì„¸ìš”!
# ì˜ˆì‹œ: "https://raw.githubusercontent.com/jh9098/stock_crawl/main/backend/output/aggregated/aggregated_stock_data.csv"
DATA_URL = "https://raw.githubusercontent.com/jh9098/stock_crawl/main/backend/output/aggregated/aggregated_stock_data.csv"

# ----- ê²½ë¡œ ì„¤ì • (ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ íŒŒì¼ìš©) -----
# ì´ íŒŒì¼(trends_dashboard.py)ì´ ìˆëŠ” ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œë¥¼ ì¡ìŠµë‹ˆë‹¤.
DASHBOARD_ROOT = os.path.dirname(os.path.abspath(__file__))
KOSPI_TXT   = os.path.join(DASHBOARD_ROOT, "ì½”ìŠ¤í”¼.txt")
KOSDAQ_TXT  = os.path.join(DASHBOARD_ROOT, "ì½”ìŠ¤ë‹¥.txt")

# ----- ë””í´íŠ¸ ê°’ -----
DEFAULT_TOP_N_KEY_ORG = 15
DEFAULT_TOP_N_STOCKS  = 15
DEFAULT_RECENT_DAYS   = 7
DEFAULT_PREV_DAYS     = 7
EPS = 1e-6

STOP_KEYWORDS = {
    "í•œêµ­", "ì •ë¶€", "ì •ì±…", "ë°œí‘œ", "ê´€ë ¨", "ì‹œì¥", "ì¦ì‹œ", "ê²½ì œ", "ì£¼ì‹", "ì´ë‚ ",
    "ê¸°ì‚¬", "ë¶„ì„", "ì—…ê³„", "íšŒì‚¬", "ê¸ˆìœµ", "íˆ¬ì", "ì‹¤ì "
}

# =========================== ìœ í‹¸ í•¨ìˆ˜ ===========================
def safe_literal_eval(val):
    """CSVì—ì„œ ì½ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë¬¸ìì—´ -> ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    try:
        # nan ê°™ì€ float íƒ€ì…ì´ ë“¤ì–´ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¬¸ìì—´ë¡œ ë¨¼ì € ë³€í™˜
        return ast.literal_eval(str(val))
    except (ValueError, SyntaxError, TypeError):
        return []

@st.cache_data(ttl=600)
def load_data_from_local(file_path):
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        # ë‚ ì§œ ì²˜ë¦¬
        published_dt = pd.to_datetime(df['published_at'], errors='coerce')
        crawled_dt = pd.to_datetime(df['crawled_at'], errors='coerce') if 'crawled_at' in df.columns else None
        df['analysis_date'] = np.where(pd.notna(published_dt), published_dt, crawled_dt)
        df['analysis_date'] = pd.to_datetime(df['analysis_date']).dt.date
        df.dropna(subset=['analysis_date'], inplace=True)
        # ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ ì²˜ë¦¬
        df['analysis_keywords'] = df['analysis_keywords'].apply(safe_literal_eval)
        df['analysis_orgs'] = df['analysis_orgs'].apply(safe_literal_eval)
        return df
    except Exception as e:
        st.error(f"ë¡œì»¬ íŒŒì¼ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.info("íŒŒì¼ ê²½ë¡œì™€ ì¸ì½”ë”©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None

# ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì…ë ¥ (ì—¬ê¸°ë§Œ ìˆ˜ì •!)
LOCAL_CSV_PATH = r"P:\stock_crawl\backend\output\merged_no_duplicate.csv"
df = load_data_from_local(LOCAL_CSV_PATH)


@st.cache_data
def load_stock_names(kospi_path: str, kosdaq_path: str):
    """ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì¢…ëª©ëª… ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
    names = []
    for p in [kospi_path, kosdaq_path]:
        try:
            # ì›¹ ë°°í¬ í™˜ê²½ì„ ê³ ë ¤í•˜ì—¬ Path ì‚¬ìš©
            txt_path = Path(p)
            if txt_path.is_file():
                txt = txt_path.read_text(encoding='utf-8').splitlines()
                names.extend([x.strip() for x in txt if x.strip()])
            else:
                 st.warning(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {p}")
        except Exception as e:
            st.warning(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ({p}): {e}")
    return sorted(set(names))

def extract_stock_mentions(org_list, stock_set):
    """ë¶„ì„ëœ ê¸°ê´€/ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ì¤‘ ì¢…ëª©ëª…ë§Œ ì¶”ì¶œ"""
    if not isinstance(org_list, list):
        return []
    # analysis_orgsì— ìˆëŠ” ì´ë¦„ì´ stock_setì— í¬í•¨ë˜ëŠ” ê²½ìš°ë§Œ í•„í„°ë§
    return [org for org in org_list if org in stock_set]


# =========================== ë°ì´í„° ë¡œë“œ ===========================
df = load_data_from_local(LOCAL_CSV_PATH)
stock_list = load_stock_names(KOSPI_TXT, KOSDAQ_TXT)
stock_set  = set(stock_list)

st.title("ğŸ“ˆ ë‰´ìŠ¤ íŠ¸ë Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ (ìë™ ì—…ë°ì´íŠ¸)")

if df is None or df.empty:
    st.warning("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. GitHub Actionsê°€ ì•„ì§ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    st.stop()

# =========================== ì‚¬ì´ë“œë°” í•„í„° ===========================
st.sidebar.header("ğŸ“Š ê¸°ë³¸ í•„í„°")

min_date = df['analysis_date'].min()
max_date = df['analysis_date'].max()

date_range = st.sidebar.date_input(
    "ë‚ ì§œ ë²”ìœ„ ì„ íƒ",
    value=(min_date, max_date), min_value=min_date, max_value=max_date
)

if len(date_range) != 2:
    st.stop()

start_date, end_date = date_range
filtered_df = df[(df['analysis_date'] >= start_date) & (df['analysis_date'] <= end_date)].copy()

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ”¤ í‘œì‹œ ê°œìˆ˜ ì„¤ì •")
TOP_N_KEY_ORG = st.sidebar.number_input("í‚¤ì›Œë“œ/ê¸°ê´€ ìƒìœ„ ê°œìˆ˜", 5, 50, DEFAULT_TOP_N_KEY_ORG, 1)
TOP_N_STOCKS = st.sidebar.number_input("ì‹œê³„ì—´ ìƒìœ„ ì¢…ëª© ê°œìˆ˜", 5, 50, DEFAULT_TOP_N_STOCKS, 1)

st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ”¥ ëª¨ë©˜í…€ ë¶„ì„ ì„¤ì •")
recent_days = st.sidebar.number_input("ìµœê·¼ Nì¼", 3, 30, DEFAULT_RECENT_DAYS, 1)
prev_days   = st.sidebar.number_input("ì§ì „ Nì¼", 3, 30, DEFAULT_PREV_DAYS, 1)

# ... (ê¸°ì¡´ ì½”ë“œ ë™ì¼, ìœ„ ìƒëµ) ...

# ===================== ë°ì´í„° ìš”ì•½ ë° ê¸°ë³¸ ì¤€ë¹„ =====================
st.success(f"ì´ **{len(filtered_df)}ê°œ ê¸°ì‚¬** ë¶„ì„ (ê¸°ê°„: {start_date} ~ {end_date})")

if not stock_set:
    st.warning("ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. txt íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    filtered_df['stock_mentions'] = filtered_df['analysis_orgs'].apply(lambda orgs: extract_stock_mentions(orgs, stock_set))

# ===================== 1. ì‹œê³„ì—´ íŠ¸ë Œë“œ(í‚¤ì›Œë“œ/ê¸°ê´€/ì¢…ëª©) =====================
st.markdown("---")
st.header("ğŸ“Š ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„")

st.subheader("ğŸ—“ï¸ ì£¼ìš” í‚¤ì›Œë“œ")
exploded_kw = filtered_df.explode('analysis_keywords').dropna(subset=['analysis_keywords'])
exploded_kw = exploded_kw[~exploded_kw['analysis_keywords'].isin(STOP_KEYWORDS | stock_set)]
if not exploded_kw.empty:
    daily_counts_kw = exploded_kw.groupby(['analysis_date', 'analysis_keywords']).size().reset_index(name='count')
    top_kw = daily_counts_kw.groupby('analysis_keywords')['count'].sum().nlargest(TOP_N_KEY_ORG).index
    top_kw_df = daily_counts_kw[daily_counts_kw['analysis_keywords'].isin(top_kw)]
    if not top_kw_df.empty:
        fig_kw = px.line(top_kw_df, x='analysis_date', y='count', color='analysis_keywords', markers=True, title=f"ìƒìœ„ {TOP_N_KEY_ORG} í‚¤ì›Œë“œ ì–¸ê¸‰ ì¶”ì´")
        st.plotly_chart(fig_kw, use_container_width=True)

st.subheader("ğŸ¢ ì£¼ìš” ê¸°ê´€ (Non-stock)")
exploded_org = filtered_df.explode('analysis_orgs').dropna(subset=['analysis_orgs'])
exploded_org = exploded_org[~exploded_org['analysis_orgs'].isin(stock_set)]
if not exploded_org.empty:
    daily_counts_org = exploded_org.groupby(['analysis_date', 'analysis_orgs']).size().reset_index(name='count')
    top_org = daily_counts_org.groupby('analysis_orgs')['count'].sum().nlargest(TOP_N_KEY_ORG).index
    top_org_df = daily_counts_org[daily_counts_org['analysis_orgs'].isin(top_org)]
    if not top_org_df.empty:
        fig_org = px.line(top_org_df, x='analysis_date', y='count', color='analysis_orgs', markers=True, title=f"ìƒìœ„ {TOP_N_KEY_ORG} ê¸°ê´€ ì–¸ê¸‰ ì¶”ì´")
        st.plotly_chart(fig_org, use_container_width=True)

st.subheader("ğŸ“ˆ ì£¼ìš” ì¢…ëª©")
exploded_stock = filtered_df.explode('stock_mentions').dropna(subset=['stock_mentions'])
if not exploded_stock.empty:
    daily_counts_stock = exploded_stock.groupby(['analysis_date', 'stock_mentions']).size().reset_index(name='count')
    top_stock = daily_counts_stock.groupby('stock_mentions')['count'].sum().nlargest(TOP_N_STOCKS).index
    top_stock_df = daily_counts_stock[daily_counts_stock['stock_mentions'].isin(top_stock)]
    if not top_stock_df.empty:
        fig_stock = px.line(top_stock_df, x='analysis_date', y='count', color='stock_mentions', markers=True, title=f"ìƒìœ„ {TOP_N_STOCKS} ì¢…ëª© ì–¸ê¸‰ ì¶”ì´")
        st.plotly_chart(fig_stock, use_container_width=True)

# ===================== 2. ê¸‰ìƒìŠ¹(íŠ¸ë Œë”©) í‚¤ì›Œë“œ/ì¢…ëª© ë¶„ì„ =====================
st.markdown("---")
st.header("ğŸš€ ìµœê·¼ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ/ì¢…ëª©/ì´ìŠˆ ë¶„ì„")

import collections
from datetime import datetime, timedelta

def flatten_keywords(series):
    # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ â†’ ì§„ì§œ ë¦¬ìŠ¤íŠ¸ë¡œ í‰íƒ„í™”
    result = []
    for x in series:
        try:
            result.extend([k for k in ast.literal_eval(str(x)) if isinstance(k, str)])
        except Exception:
            continue
    return result

now = pd.Timestamp.now()
recent_limit = now - pd.Timedelta(days=recent_days)
prev_limit = recent_limit - pd.Timedelta(days=prev_days)

recent_df = filtered_df[filtered_df['analysis_date'] >= recent_limit.date()]
prev_df   = filtered_df[(filtered_df['analysis_date'] < recent_limit.date()) & (filtered_df['analysis_date'] >= prev_limit.date())]

recent_kw = collections.Counter(flatten_keywords(recent_df['analysis_keywords']))
prev_kw   = collections.Counter(flatten_keywords(prev_df['analysis_keywords']))

trending = []
for k in recent_kw:
    prev_count = prev_kw.get(k, 0)
    if recent_kw[k] >= 3 and recent_kw[k] > prev_count:  # ìµœì†Œ ë“±ì¥ íšŸìˆ˜ í•„í„°
        rate = ((recent_kw[k]-prev_count)/prev_count*100) if prev_count else 1000  # 0 ëŒ€ë¹„ëŠ” 1000%
        trending.append((k, recent_kw[k], prev_count, rate))
trending.sort(key=lambda x: x[3], reverse=True)
trend_df = pd.DataFrame(trending, columns=["í‚¤ì›Œë“œ", f"ìµœê·¼ {recent_days}ì¼", f"ì§ì „ {prev_days}ì¼", "ì¦ê°€ìœ¨(%)"])
st.subheader(f"ğŸ”¥ ìµœê·¼ {recent_days}ì¼ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œ Top 10")
st.dataframe(trend_df.head(10))

# ===================== 3. ì¢…ëª©ë³„ ê°ì„±(ê¸/ë¶€/ì¤‘) ì‹œê³„ì—´ =====================
st.markdown("---")
st.header("ğŸ˜ƒ ì¢…ëª©ë³„ ê°ì„± ì¶”ì´ (ê¸/ë¶€/ì¤‘ ì‹œê³„ì—´)")

sentiment_df = filtered_df.explode('stock_mentions').dropna(subset=['stock_mentions'])
sentiment_ts = sentiment_df.groupby(['analysis_date', 'stock_mentions', 'sentiment_label']).size().reset_index(name='count')
top_stock = sentiment_ts.groupby('stock_mentions')['count'].sum().nlargest(TOP_N_STOCKS).index
sentiment_ts = sentiment_ts[sentiment_ts['stock_mentions'].isin(top_stock)]

if not sentiment_ts.empty:
    fig = px.line(
        sentiment_ts,
        x='analysis_date', y='count',
        color='sentiment_label',
        facet_row='stock_mentions',
        markers=True,
        title='ìƒìœ„ ì¢…ëª©ë³„ ê°ì„± ì¶”ì´'
    )
    st.plotly_chart(fig, use_container_width=True)

# ===================== 4. ê°ì„±ë¶„ì„ ë¹„ìœ¨ (ê¸/ë¶€/ì¤‘) ì „ì²´ ìš”ì•½ =====================
st.markdown("---")
st.header("ğŸ§  ì „ì²´ ê°ì„± ë¶„í¬ (ê¸/ë¶€/ì¤‘)")

sent_count = filtered_df['sentiment_label'].value_counts()
st.write("ê¸°ì‚¬ ì „ì²´ ê°ì„± ë¶„í¬ (ê±´ìˆ˜ ê¸°ì¤€):")
st.bar_chart(sent_count)

# ===================== 5. ìµœê·¼ í…Œë§ˆ/ì´ìŠˆ í´ëŸ¬ìŠ¤í„° ë° ìš”ì•½(ê¸°ì´ˆ) =====================
st.markdown("---")
st.header("ğŸ” ìµœê·¼ í…Œë§ˆë³„ ê¸°ì‚¬ ì§‘ê³„ (ê¸°ì´ˆ í´ëŸ¬ìŠ¤í„°ë§)")

theme_df = exploded_kw.groupby(['analysis_keywords'])['url'].count().sort_values(ascending=False).reset_index()
theme_df = theme_df[~theme_df['analysis_keywords'].isin(STOP_KEYWORDS | stock_set)]
st.write("ìµœê·¼ ê¸°ì‚¬ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ì´ìŠˆ/í…Œë§ˆ Top 20")
st.dataframe(theme_df.head(20))

# ===================== 6. ì •ì±…/ì œë„/ë¦¬ìŠ¤í¬ ì´ìŠˆ ë‰´ìŠ¤ í•„í„° =====================
st.markdown("---")
st.header("âš ï¸ ì •ì±…/ì œë„/ë¦¬ìŠ¤í¬ ê´€ë ¨ ë‰´ìŠ¤")

policy_words = {"ì •ì±…", "ê·œì œ", "ë²•ì•ˆ", "ì„¸ì œ", "ê¸ˆë¦¬", "ì •ë¶€", "ë‹¹êµ­", "ê³µì‹œ", "ë°œí‘œ", "ë¦¬ìŠ¤í¬", "ìœ„ê¸°"}
policy_mask = exploded_kw['analysis_keywords'].apply(lambda x: any(pw in str(x) for pw in policy_words))
policy_news = exploded_kw[policy_mask][['analysis_date', 'url', 'analysis_keywords']].drop_duplicates().head(50)
st.write("ì •ì±…/ì œë„/ë¦¬ìŠ¤í¬ ê´€ë ¨ ìµœê·¼ ë‰´ìŠ¤ Top 50")
st.dataframe(policy_news)

# ===================== 7. ê¸°ì‚¬ë³„ ìš”ì•½Â·AI ì½”ë©˜íŠ¸/íˆ¬ìí¬ì¸íŠ¸ =====================
st.markdown("---")
st.header("ğŸ’¡ AI ê¸°ì‚¬ ìš”ì•½/íˆ¬ìí¬ì¸íŠ¸/í•œì¤„í‰")

ai_summary = filtered_df[['analysis_date', 'url', 'summary_ai', 'sentiment_label']].sort_values('analysis_date', ascending=False).head(20)
st.write("ìµœì‹  ê¸°ì‚¬ í•œì¤„ ìš”ì•½/AI ì½”ë©˜íŠ¸ (ìµœì‹ ìˆœ Top 20)")
st.dataframe(ai_summary)

# ===================== 8. ìœ ì‚¬ë‰´ìŠ¤/ì—°ê´€ ì´ìŠˆ ì¶”ì²œ (ê¸°ì´ˆ, í‚¤ì›Œë“œ ê¸°ë°˜) =====================
st.markdown("---")
st.header("ğŸ”— ì—°ê´€/ìœ ì‚¬ ë‰´ìŠ¤ ì¶”ì²œ (í‚¤ì›Œë“œ ê¸°ë°˜)")

def find_related_news(keyword, df, topn=10):
    # ì…ë ¥ í‚¤ì›Œë“œì™€ ì—°ê´€ëœ ê¸°ì‚¬ ìµœì‹ ìˆœ ì¶”ì²œ
    mask = df['analysis_keywords'].apply(lambda kws: keyword in kws if isinstance(kws, list) else False)
    return df[mask][['analysis_date', 'url', 'summary_ai', 'sentiment_label']].sort_values('analysis_date', ascending=False).head(topn)

user_kw = st.text_input("ì—°ê´€ ë‰´ìŠ¤ ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì…ë ¥(ì˜ˆ: '2ì°¨ì „ì§€', 'AIë°˜ë„ì²´')")
if user_kw:
    st.write(f"'{user_kw}' ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ Top 10:")
    st.dataframe(find_related_news(user_kw, filtered_df))

# ===================== ë =====================
