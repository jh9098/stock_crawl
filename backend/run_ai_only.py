# backend/run_ai_only.py
# -*- coding: utf-8 -*-
# ëª©ì : ì´ë¯¸ ìˆ˜ì§‘ëœ CSV íŒŒì¼ì„ ì½ì–´ AI ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ëŠ” ì„ì‹œ ìŠ¤í¬ë¦½íŠ¸

import os
import sys
import json
import pandas as pd
import google.generativeai as genai
from datetime import datetime, timedelta

# --- ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ AI ë¶„ì„ì— í•„ìš”í•œ í•¨ìˆ˜ë§Œ ê°€ì ¸ì˜´ ---

# ì„¤ì • ì˜ì—­ (ì¼ë¶€ë§Œ í•„ìš”)
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
BATCH_SIZE = 5

gemini_model = None

def initialize_gemini_model():
    """Gemini ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global gemini_model
    if not GOOGLE_API_KEY:
        raise ValueError("Google API í‚¤ê°€ GitHub Secretsì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        gemini_model = genai.GenerativeModel("models/gemini-1.5-flash")
        print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        print(f"ğŸš¨ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def get_stock_analysis_prompt(content):
    """ì£¼ì‹/ê²½ì œ ë‰´ìŠ¤ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"""
ë‹¹ì‹ ì€ ìµœê³ ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì— ì œê³µë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬, ê° ê¸°ì‚¬ë³„ë¡œ ì§€ì •ëœ JSON í˜•ì‹ì— ë§ì¶° ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ê·œì¹™]
- ê° ê¸°ì‚¬ëŠ” `<article>` íƒœê·¸ë¡œ êµ¬ë¶„ë˜ë©°, ê° ê¸°ì‚¬ì˜ `<id>`ë¥¼ JSON ê²°ê³¼ì˜ "id" í•„ë“œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
- `analysis_keywords`: ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë¥¼ 5ê°œ ë‚´ì™¸ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `analysis_orgs`: ê¸°ì‚¬ì— ì–¸ê¸‰ëœ ì£¼ìš” 'ê¸°ê´€/ê¸°ì—…(ORG)'ì„ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `summary_ai`: ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
- `sentiment_label`: ê¸°ì‚¬ì˜ ì „ë°˜ì ì¸ í†¤ì´ ê¸ì •ì ì¸ì§€(Positive), ë¶€ì •ì ì¸ì§€(Negative), ì¤‘ë¦½ì ì¸ì§€(Neutral) í‰ê°€í•©ë‹ˆë‹¤.
- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë‹¨ì¼ JSON ë¦¬ìŠ¤íŠ¸(ë°°ì—´) í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ì½”ë“œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

[ë¶„ì„í•  ê¸°ì‚¬ ëª©ë¡]
{content}

[ì¶œë ¥ JSON í˜•ì‹]
```json
[
  {{
    "id": "<article>ì˜ id ê°’>",
    "analysis_keywords": ["í‚¤ì›Œë“œ1", ...],
    "analysis_orgs": ["ê¸°ê´€1", ...],
    "summary_ai": "ê¸°ì‚¬ ìš”ì•½",
    "sentiment_label": "Positive, Negative, ë˜ëŠ” Neutral"
  }},
  ...
]"""
def analyze_articles_with_ai(articles):
    """ê¸°ì‚¬ ëª©ë¡ì„ AIë¥¼ í†µí•´ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    print("\n--- AI êµ¬ì¡°í™” ë¶„ì„ ì‹œì‘ ---")
    articles_to_process, article_map = [], {}
    for i, article in enumerate(articles):
        article['unique_id'] = f"art_{i}"
        article_map[article['unique_id']] = article
        # CSVì—ì„œ ì½ì–´ì˜¨ contentê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°(NaN)ë¥¼ ëŒ€ë¹„í•´ ë¬¸ìì—´ë¡œ ë³€í™˜
        content_to_analyze = str(article.get("content", ""))
        if not content_to_analyze or content_to_analyze.startswith(("[ì‹¤íŒ¨]", "[ì˜¤ë¥˜]")):
            content_to_analyze = str(article.get("summary", ""))
        if content_to_analyze:
            article['content_to_analyze'] = content_to_analyze
            articles_to_process.append(article)
    print(f"  - AI ë¶„ì„ ëŒ€ìƒ: {len(articles_to_process)}ê°œ / ì´ {len(articles)}ê°œ")
    if not articles_to_process:
        return articles

    total_batches = (len(articles_to_process) + BATCH_SIZE - 1) // BATCH_SIZE
    for i in range(0, len(articles_to_process), BATCH_SIZE):
        batch = articles_to_process[i:i + BATCH_SIZE]
        current_batch_num = i // BATCH_SIZE + 1
        print(f"  - ë°°ì¹˜ {current_batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
        batch_content = "\n\n".join([
            f"<article>\n<id>{art['unique_id']}</id>\n<content>\n{art['content_to_analyze']}\n</content>\n</article>"
            for art in batch
        ])
        final_prompt = get_stock_analysis_prompt(batch_content)

        try:
            response = gemini_model.generate_content(final_prompt)
            cleaned_response = response.text.strip().lstrip("```json").lstrip("```").rstrip("```")
            analysis_results_list = json.loads(cleaned_response)
            if isinstance(analysis_results_list, list):
                for result in analysis_results_list:
                    if result.get('id') in article_map:
                        article_map[result['id']].update(result)
            else:
                print(f"    âš ï¸ ë°°ì¹˜ {current_batch_num} ë¶„ì„ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜.")
        except Exception as e:
            print(f"    - ë°°ì¹˜ {current_batch_num} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    final_list = list(article_map.values())
    for art in final_list:
        art.pop('unique_id', None)
        art.pop('id', None)
        art.pop('content_to_analyze', None)
    print("--- âœ… AI ë¶„ì„ ì™„ë£Œ ---")
    return final_list

def aggregate_and_save_to_csv(new_articles, output_dir):
    """ë¶„ì„ ì™„ë£Œëœ ê¸°ì‚¬ë¥¼ ìµœì¢… CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\n--- ìµœì¢… ë°ì´í„° ì €ì¥ ì‹œì‘ ---")
    if not new_articles:
        print(" - ì €ì¥í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    df = pd.DataFrame(new_articles)

    # CSVì— ì €ì¥í•˜ê¸° ì¢‹ê²Œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    for col in ['analysis_orgs', 'analysis_keywords']:
        if col in df.columns:
            # NaN ê°’ì´ ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹„ì–´ìˆëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸('[]')ë¡œ ë³€í™˜
            df[col] = df[col].apply(lambda x: str(x) if isinstance(x, list) else '[]')

    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "aggregated_stock_data.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"--- âœ… CSV ì €ì¥ ì™„ë£Œ. ì´ {len(df)}ê°œ ê¸°ì‚¬ ì €ì¥ ---")
    print(f"   - ì €ì¥ ê²½ë¡œ: {csv_path}")

def main():
    """
    ì €ì¥ëœ CSV íŒŒì¼ì„ ì½ì–´ AI ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("="*50)
    print(" K-Stock News AI Analysis Only - START")
    print("="*50)
    # ë³¸ë¬¸ ì¶”ì¶œê¹Œì§€ ì™„ë£Œëœ ì…ë ¥ íŒŒì¼ ê²½ë¡œ
    input_csv_path = "backend/output/intermediate/crawled_data.csv"
    # ìµœì¢… ê²°ê³¼ë¬¼ì´ ì €ì¥ë  í´ë” ê²½ë¡œ
    output_dir = os.path.join("backend", "output", "aggregated")

    # 1. AI ëª¨ë¸ ì´ˆê¸°í™”
    try:
        initialize_gemini_model()
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    # 2. CSV íŒŒì¼ ì½ê¸°
    print(f"\n--- ë°ì´í„° ë¡œë“œ ì‹œì‘: {input_csv_path} ---")
    if not os.path.exists(input_csv_path):
        print(f"ğŸš¨ ì…ë ¥ íŒŒì¼({input_csv_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)

    df = pd.read_csv(input_csv_path)
    # pandasê°€ CSVë¥¼ ì½ì„ ë•Œ ë¹ˆ ì…€ì„ NaNìœ¼ë¡œ ì½ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´
    df = df.fillna('') 
    articles_to_analyze = df.to_dict('records')
    print(f"âœ… {len(articles_to_analyze)}ê°œì˜ ê¸°ì‚¬ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    # 3. AI ë¶„ì„ ì‹¤í–‰
    analyzed_articles = analyze_articles_with_ai(articles_to_analyze)

    # 4. ìµœì¢… ê²°ê³¼ ì €ì¥
    aggregate_and_save_to_csv(analyzed_articles, output_dir)

    print("\n" + "="*50)
    print(" K-Stock News AI Analysis Only - COMPLETE")
    print("="*50)

if __name__ == "__main__":
    main()
