# backend/run_pipeline.py
# -*- coding: utf-8 -*-

import os
import sys
import json
import time
from datetime import datetime, timedelta
import re
import warnings
import urllib3

# --- í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import requests
import pandas as pd
from bs4 import BeautifulSoup
import google.generativeai as genai
import google.api_core.exceptions

# --- SSL ê²½ê³  ë¹„í™œì„±í™” ---
warnings.filterwarnings("ignore")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==============================================================================
# ğŸš€ ì„¤ì • ì˜ì—­
# ==============================================================================

# --- API í‚¤ ë° ID (GitHub Secretsë¥¼ í†µí•´ í™˜ê²½ ë³€ìˆ˜ë¡œ ì „ë‹¬ë°›ìŒ) ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
JSONBIN_API_KEY = os.getenv("JSONBIN_API_KEY")
JSONBIN_BIN_ID = os.getenv("JSONBIN_BIN_ID")

# --- ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡ ---
STOCK_SEARCH_KEYWORDS = [
    "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "í™˜ìœ¨", "ê¸ˆë¦¬ì¸ìƒ", "FOMC", "ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜", "ë°˜ë„ì²´",
    "HBM", "AIë°˜ë„ì²´", "2ì°¨ì „ì§€", "ë°”ì´ì˜¤", "ì œì•½", "ë°¸ë¥˜ì—…", "ê¸°ì—… ì‹¤ì "
]

# --- ê¸°íƒ€ ì„¤ì • ---
RATE_LIMIT_DELAY = 1
BATCH_SIZE = 5
ARTICLE_END_MARKERS = [
    "ë¬´ë‹¨ì „ì¬", "ë¬´ë‹¨ ì „ì¬", "ì¬ë°°í¬ ê¸ˆì§€", "ì €ì‘ê¶Œì", "ê´‘ê³ ë¬¸ì˜", "ê¸°ì=", "ê¸°ì (",
    "ê¸°ì]", "[ì‚¬ì§„=", "(ì„œìš¸=ì—°í•©ë‰´ìŠ¤)", "[â“’", "AIí•™ìŠµ ì´ìš© ê¸ˆì§€", "Copyright"
]

# ==============================================================================
# ğŸ¤– AI ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜
# ==============================================================================
gemini_model = None

def initialize_gemini_model():
    """Gemini ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global gemini_model
    if not GOOGLE_API_KEY:
        raise ValueError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        gemini_model = genai.GenerativeModel("models/gemini-1.5-flash")
        print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        print(f"ğŸš¨ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def get_stock_analysis_prompt(content):
    """ì£¼ì‹/ê²½ì œ ë‰´ìŠ¤ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"""
ë‹¹ì‹ ì€ ìµœê³ ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì— ì œê³µë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬, ê° ê¸°ì‚¬ë³„ë¡œ ì§€ì •ëœ JSON í˜•ì‹ì— ë§ì¶° ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ê·œì¹™]
- ê° ê¸°ì‚¬ëŠ” `<article>` íƒœê·¸ë¡œ êµ¬ë¶„ë˜ë©°, ê° ê¸°ì‚¬ì˜ `<id>`ë¥¼ JSON ê²°ê³¼ì˜ "id" í•„ë“œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
- `analysis_keywords`: ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë¥¼ 5ê°œ ë‚´ì™¸ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `analysis_orgs`: ê¸°ì‚¬ì— ì–¸ê¸‰ëœ ì£¼ìš” 'ê¸°ê´€/ê¸°ì—…(ORG)'ì„ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `summary_ai`: ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
- `sentiment_label`: ê¸°ì‚¬ì˜ ì „ë°˜ì ì¸ í†¤ì´ ê¸ì •ì ì¸ì§€(Positive), ë¶€ì •ì ì¸ì§€(Negative), ì¤‘ë¦½ì ì¸ì§€(Neutral) í‰ê°€í•©ë‹ˆë‹¤.
- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë‹¨ì¼ JSON ë¦¬ìŠ¤íŠ¸(ë°°ì—´) í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ì½”ë“œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

[ë¶„ì„í•  ê¸°ì‚¬ ëª©ë¡]
{content}

[ì¶œë ¥ JSON í˜•ì‹]
```json
[
  {{
    "id": "<article>ì˜ id ê°’>",
    "analysis_keywords": ["í‚¤ì›Œë“œ1", ...],
    "analysis_orgs": ["ê¸°ê´€1", ...],
    "summary_ai": "ê¸°ì‚¬ ìš”ì•½",
    "sentiment_label": "Positive, Negative, ë˜ëŠ” Neutral"
  }},
  ...
]
"""
def crawl_naver_news(keywords, existing_urls):
    """ì§€ì •ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    api_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    all_new_articles = []
    today = datetime.now()
    # [ìˆ˜ì •] ìµœê·¼ 3ì¼ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ë²”ìœ„ í™•ì¥
    target_dates = [(today - timedelta(days=i)).date() for i in range(3)]
    date_str = ", ".join([d.strftime('%Y-%m-%d') for d in target_dates])
    print(f"\n--- 1ë‹¨ê³„: ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ëŒ€ìƒ ë‚ ì§œ: {date_str}) ---")
    for keyword in keywords:
        print(f" ğŸ” í‚¤ì›Œë“œ '{keyword}' ìˆ˜ì§‘ ì¤‘...")
        params = {"query": keyword, "display": 100, "start": 1, "sort": "date"}
        try:
            response = requests.get(api_url, headers=headers, params=params, verify=False, timeout=10)
            response.raise_for_status()
            data = response.json()
            items = data.get('items', [])
            if not items:
                continue
            for item in items:
                try:
                    pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S %z').date()
                except (ValueError, TypeError):
                    continue
                if pub_date not in target_dates:
                    continue
                url = item.get('originallink') or item.get('link')
                if not url or url in existing_urls:
                    continue
                title = re.sub('<[^<]+?>', '', item.get('title', ''))
                summary = re.sub('<[^<]+?>', '', item.get('description', ''))
                all_new_articles.append({
                    "search_keyword": keyword, "url": url, "title": title, "summary": summary,
                    "crawled_at": datetime.now().isoformat(), "published_at": pub_date.strftime('%Y-%m-%d')
                })
                existing_urls.add(url)
            time.sleep(RATE_LIMIT_DELAY)
        except Exception as e:
            print(f" âŒ '{keyword}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    print(f"--- âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(all_new_articles)}ê°œì˜ ìƒˆ ê¸°ì‚¬ ë°œê²¬ ---")
    return all_new_articles

def extract_article_content(url):
    """ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    headers = {"User-Agent": "Mozilla/5.0", "Accept-Language": "ko-KR,ko;q=0.9"}
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        if response.encoding.lower() in ['iso-8859-1', 'euc-kr']:
            response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "lxml")
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'iframe', 'figure']):
            tag.decompose()
        selectors = [
            "#article-view-content-div", "#CmAdContent", "#articleBody", "#article-body", "#view_content_wrap",
            "#article-content-body", "#news_body_area", "#article_content", "#news-contents", "#articleText",
            "article", ".article_body", ".news_end"
        ]
        content_area = next((soup.select_one(s) for s in selectors if soup.select_one(s)), None)
        if content_area:
            text = content_area.get_text(separator='\n', strip=True)
            if len(text) > 100:
                lines = text.split('\n')
                cleaned_lines = []
                for line in lines:
                    if any(marker in line for marker in ARTICLE_END_MARKERS):
                        break
                    cleaned_lines.append(line)
                return '\n'.join(cleaned_lines).strip()
        return "[ì‹¤íŒ¨] ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ ì‹¤íŒ¨"
    except Exception as e:
        return f"[ì˜¤ë¥˜] {str(e)}"

def analyze_articles_with_ai(articles):
    """ê¸°ì‚¬ ëª©ë¡ì„ AIë¥¼ í†µí•´ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    print("\n--- 3ë‹¨ê³„: AI êµ¬ì¡°í™” ë¶„ì„ ì‹œì‘ ---")
    articles_to_process, article_map = [], {}
    for i, article in enumerate(articles):
        article['unique_id'] = f"art_{i}"
        article_map[article['unique_id']] = article
        content_to_analyze = article.get("content", "")
        if not content_to_analyze or content_to_analyze.startswith(("[ì‹¤íŒ¨]", "[ì˜¤ë¥˜]")):
            content_to_analyze = article.get("summary", "")
        if content_to_analyze:
            article['content_to_analyze'] = content_to_analyze
            articles_to_process.append(article)
    print(f"  - AI ë¶„ì„ ëŒ€ìƒ: {len(articles_to_process)}ê°œ / ì´ {len(articles)}ê°œ")
    if not articles_to_process:
        return articles

    for i in range(0, len(articles_to_process), BATCH_SIZE):
        batch = articles_to_process[i:i + BATCH_SIZE]
        print(f"  - ë°°ì¹˜ {i//BATCH_SIZE + 1} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
        batch_content = "\n\n".join([
            f"<article>\n<id>{art['unique_id']}</id>\n<content>\n{art['content_to_analyze']}\n</content>\n</article>"
            for art in batch
        ])
        final_prompt = get_stock_analysis_prompt(batch_content)

        try:
            response = gemini_model.generate_content(final_prompt)
            cleaned_response = response.text.strip().lstrip("```json").lstrip("```").rstrip("```")
            analysis_results_list = json.loads(cleaned_response)
            if isinstance(analysis_results_list, list):
                for result in analysis_results_list:
                    if result.get('id') in article_map:
                        article_map[result['id']].update(result)
            else:
                print(f"    âš ï¸ ë°°ì¹˜ {i//BATCH_SIZE + 1} ë¶„ì„ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜.")
        except Exception as e:
            print(f"    - ë°°ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    final_list = list(article_map.values())
    for art in final_list:
        art.pop('unique_id', None)
        art.pop('id', None)
        art.pop('content_to_analyze', None)
    print("--- âœ… AI ë¶„ì„ ì™„ë£Œ ---")
    return final_list

def get_existing_data_from_bin():
    """JSONBinì—ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. í•­ìƒ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤."""
    print("  - ì›ê²© ì €ì¥ì†Œ(JSONBin.io)ì—ì„œ ê¸°ì¡´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    headers = {'X-Master-Key': JSONBIN_API_KEY, 'X-Bin-Versioning': 'false'}
    url = f"https://api.jsonbin.io/v3/b/{JSONBIN_BIN_ID}/latest"
    try:
        req = requests.get(url, headers=headers, timeout=15)
        req.raise_for_status()
        response_data = req.json()

        # [í•µì‹¬ ìˆ˜ì •] JSONBin.io ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶° ì•ˆì •ì ìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ
        # ìµœìƒìœ„ê°€ ë”•ì…”ë„ˆë¦¬ì´ê³  'record' í‚¤ê°€ ìˆìœ¼ë©´ ê·¸ ì•ˆì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©
        if isinstance(response_data, dict) and 'record' in response_data:
            result = response_data['record']
        # ìµœìƒìœ„ê°€ ë¦¬ìŠ¤íŠ¸ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        elif isinstance(response_data, list):
            result = response_data
        # ê·¸ ì™¸ì˜ ê²½ìš° ë¹„ì •ìƒìœ¼ë¡œ ê°„ì£¼
        else:
            print("  - âš ï¸ ì›ê²© ë°ì´í„°ê°€ ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì…ë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            result = []

        # ìµœì¢… ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í•œë²ˆ ë” í™•ì¸
        if not isinstance(result, list):
             print(f"  - âš ï¸ ìµœì¢… ì¶”ì¶œ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤ (íƒ€ì…: {type(result)}). ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
             return []

        print(f"  - ê¸°ì¡´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ. ({len(result)}ê°œ)")
        return result
        
    except json.JSONDecodeError:
        print("  - âš ï¸ ì›ê²© ë°ì´í„°ê°€ ë¹„ì–´ ìˆê±°ë‚˜ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return []
    except Exception as e:
        print(f"  - âš ï¸ ê¸°ì¡´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì•„ë§ˆë„ ì²« ì‹¤í–‰): {e}")
        return []
def upload_data_to_bin(data):
    """JSONBinì— ìµœì¢… ë°ì´í„°ë¥¼ ì—…ë¡œë“œ(ë®ì–´ì“°ê¸°)í•©ë‹ˆë‹¤."""
    print(" - ì›ê²© ì €ì¥ì†Œ(JSONBin.io)ì— ìµœì¢… ë°ì´í„° ì—…ë¡œë“œ ì¤‘...")
    headers = {'Content-Type': 'application/json', 'X-Master-Key': JSONBIN_API_KEY}
    url = f"https://api.jsonbin.io/v3/b/{JSONBIN_BIN_ID}"
    try:
        req = requests.put(url, json=data, headers=headers, timeout=15)
        req.raise_for_status()
        print(f" - âœ… ìµœì¢… ë°ì´í„° ì—…ë¡œë“œ ì„±ê³µ! (ì´ {len(data)}ê°œ)")
    except Exception as e:
        print(f" - ğŸš¨ ìµœì¢… ë°ì´í„° ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

def aggregate_data(new_articles):
    """ìƒˆë¡œìš´ ê¸°ì‚¬ì™€ ê¸°ì¡´ ê¸°ì‚¬ë¥¼ ë³‘í•©í•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤."""
    print("\n--- 4ë‹¨ê³„: ë°ì´í„° ë³‘í•© ë° ì—…ë¡œë“œ ì‹œì‘ ---")
    if not new_articles:
        print(" - ì·¨í•©í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    # 1. ì›ê²© ì €ì¥ì†Œì—ì„œ ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    existing_articles = get_existing_data_from_bin()

    # 2. DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
    df_new = pd.DataFrame(new_articles)
    df_existing = pd.DataFrame(existing_articles)

    combined_df = pd.concat([df_existing, df_new], ignore_index=True)
    combined_df.drop_duplicates(subset=['url'], keep='last', inplace=True)

    # 3. ì˜¤ë˜ëœ ë°ì´í„° ì œê±° (ì˜ˆ: ìµœê·¼ 30ì¼ì¹˜ ë°ì´í„°ë§Œ ìœ ì§€)
    thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    combined_df['published_at'] = pd.to_datetime(combined_df['published_at'], errors='coerce').dt.strftime('%Y-%m-%d')
    final_df = combined_df[combined_df['published_at'] >= thirty_days_ago]

    print(f"  - ë°ì´í„° ë³‘í•© ì™„ë£Œ. (ê¸°ì¡´: {len(df_existing)}, ì‹ ê·œ: {len(df_new)}, ì¤‘ë³µì œê±° í›„: {len(combined_df)}, ìµœì¢…: {len(final_df)})")

    # 4. ë‹¤ì‹œ JSON ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œ
    final_data = final_df.to_dict(orient='records')
    upload_data_to_bin(final_data)

def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print("="*50)
    print(" K-Stock News Analysis Pipeline (GitHub Actions) - START")
    print("="*50)
    try:
        initialize_gemini_model()
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1) # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ì‹¤í–‰ ì¤‘ë‹¨

    # ê¸°ì¡´ ë°ì´í„°(URL)ëŠ” JSONBinì—ì„œ ì§ì ‘ ê°€ì ¸ì™€ ì¤‘ë³µì„ ì²´í¬
    existing_articles = get_existing_data_from_bin()
    existing_urls = {article.get('url') for article in existing_articles}

    new_articles = crawl_naver_news(STOCK_SEARCH_KEYWORDS, existing_urls)

    if not new_articles:
        print("\nâœ… ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    print("\n--- 2ë‹¨ê³„: ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘ ---")
    for i, article in enumerate(new_articles):
        if not article.get('content'):
            print(f"  - ({i+1}/{len(new_articles)}) ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {article.get('url', '')[:70]}...")
            article['content'] = extract_article_content(article.get('url', ''))
            time.sleep(0.1)
    print("--- âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ---")

    analyzed_articles = analyze_articles_with_ai(new_articles)

    aggregate_data(analyzed_articles)

    print("\n" + "="*50)
    print(" K-Stock News Analysis Pipeline - COMPLETE")
    print("="*50)

if __name__ == "__main__":
    main()