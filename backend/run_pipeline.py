# backend/run_pipeline.py
# -*- coding: utf-8 -*-
import csv
import os
import sys
import json
import time
from datetime import datetime, timedelta
import re
import warnings
import urllib3

# --- í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import requests
import pandas as pd
from bs4 import BeautifulSoup
import google.generativeai as genai
import google.api_core.exceptions

# --- SSL ê²½ê³  ë¹„í™œì„±í™” ---
warnings.filterwarnings("ignore")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==============================================================================
# ğŸš€ ì„¤ì • ì˜ì—­
# ==============================================================================

# --- API í‚¤ ë° ID (GitHub Secretsë¥¼ í†µí•´ í™˜ê²½ ë³€ìˆ˜ë¡œ ì „ë‹¬ë°›ìŒ) ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
JSONBIN_API_KEY = os.getenv("JSONBIN_API_KEY")
JSONBIN_BIN_ID = os.getenv("JSONBIN_BIN_ID")

# --- ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡ ---
STOCK_SEARCH_KEYWORDS = [
    "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "í™˜ìœ¨", "ê¸ˆë¦¬ì¸ìƒ", "FOMC", "ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜", "ë°˜ë„ì²´",
    "HBM", "AIë°˜ë„ì²´", "2ì°¨ì „ì§€", "ë°”ì´ì˜¤", "ì œì•½", "ë°¸ë¥˜ì—…", "ê¸°ì—… ì‹¤ì "
]

# --- ê¸°íƒ€ ì„¤ì • ---
RATE_LIMIT_DELAY = 1
BATCH_SIZE = 5
ARTICLE_END_MARKERS = [
    "ë¬´ë‹¨ì „ì¬", "ë¬´ë‹¨ ì „ì¬", "ì¬ë°°í¬ ê¸ˆì§€", "ì €ì‘ê¶Œì", "ê´‘ê³ ë¬¸ì˜", "ê¸°ì=", "ê¸°ì (",
    "ê¸°ì]", "[ì‚¬ì§„=", "(ì„œìš¸=ì—°í•©ë‰´ìŠ¤)", "[â“’", "AIí•™ìŠµ ì´ìš© ê¸ˆì§€", "Copyright"
]

# ==============================================================================
# ğŸ¤– AI ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜
# ==============================================================================
gemini_model = None

def initialize_gemini_model():
    """Gemini ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global gemini_model
    if not GOOGLE_API_KEY:
        raise ValueError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        gemini_model = genai.GenerativeModel("models/gemini-1.5-flash")
        print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        print(f"ğŸš¨ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def get_stock_analysis_prompt(content):
    """ì£¼ì‹/ê²½ì œ ë‰´ìŠ¤ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"""
ë‹¹ì‹ ì€ ìµœê³ ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì— ì œê³µë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬, ê° ê¸°ì‚¬ë³„ë¡œ ì§€ì •ëœ JSON í˜•ì‹ì— ë§ì¶° ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ê·œì¹™]
- ê° ê¸°ì‚¬ëŠ” `<article>` íƒœê·¸ë¡œ êµ¬ë¶„ë˜ë©°, ê° ê¸°ì‚¬ì˜ `<id>`ë¥¼ JSON ê²°ê³¼ì˜ "id" í•„ë“œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
- `analysis_keywords`: ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë¥¼ 5ê°œ ë‚´ì™¸ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `analysis_orgs`: ê¸°ì‚¬ì— ì–¸ê¸‰ëœ ì£¼ìš” 'ê¸°ê´€/ê¸°ì—…(ORG)'ì„ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `summary_ai`: ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
- `sentiment_label`: ê¸°ì‚¬ì˜ ì „ë°˜ì ì¸ í†¤ì´ ê¸ì •ì ì¸ì§€(Positive), ë¶€ì •ì ì¸ì§€(Negative), ì¤‘ë¦½ì ì¸ì§€(Neutral) í‰ê°€í•©ë‹ˆë‹¤.
- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë‹¨ì¼ JSON ë¦¬ìŠ¤íŠ¸(ë°°ì—´) í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ì½”ë“œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

[ë¶„ì„í•  ê¸°ì‚¬ ëª©ë¡]
{content}

[ì¶œë ¥ JSON í˜•ì‹]
```json
[
  {{
    "id": "<article>ì˜ id ê°’>",
    "analysis_keywords": ["í‚¤ì›Œë“œ1", ...],
    "analysis_orgs": ["ê¸°ê´€1", ...],
    "summary_ai": "ê¸°ì‚¬ ìš”ì•½",
    "sentiment_label": "Positive, Negative, ë˜ëŠ” Neutral"
  }},
  ...
]
"""
def crawl_naver_news(keywords, existing_urls):
    """ì§€ì •ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    api_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    all_new_articles = []
    today = datetime.now()
    # [ìˆ˜ì •] ìµœê·¼ 3ì¼ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ë²”ìœ„ í™•ì¥
    target_dates = [(today - timedelta(days=i)).date() for i in range(3)]
    date_str = ", ".join([d.strftime('%Y-%m-%d') for d in target_dates])
    print(f"\n--- 1ë‹¨ê³„: ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ëŒ€ìƒ ë‚ ì§œ: {date_str}) ---")
    for keyword in keywords:
        print(f" ğŸ” í‚¤ì›Œë“œ '{keyword}' ìˆ˜ì§‘ ì¤‘...")
        params = {"query": keyword, "display": 100, "start": 1, "sort": "date"}
        try:
            response = requests.get(api_url, headers=headers, params=params, verify=False, timeout=10)
            response.raise_for_status()
            data = response.json()
            items = data.get('items', [])
            if not items:
                continue
            for item in items:
                try:
                    pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S %z').date()
                except (ValueError, TypeError):
                    continue
                if pub_date not in target_dates:
                    continue
                url = item.get('originallink') or item.get('link')
                if not url or url in existing_urls:
                    continue
                title = re.sub('<[^<]+?>', '', item.get('title', ''))
                summary = re.sub('<[^<]+?>', '', item.get('description', ''))
                all_new_articles.append({
                    "search_keyword": keyword, "url": url, "title": title, "summary": summary,
                    "crawled_at": datetime.now().isoformat(), "published_at": pub_date.strftime('%Y-%m-%d')
                })
                existing_urls.add(url)
            time.sleep(RATE_LIMIT_DELAY)
        except Exception as e:
            print(f" âŒ '{keyword}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    print(f"--- âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(all_new_articles)}ê°œì˜ ìƒˆ ê¸°ì‚¬ ë°œê²¬ ---")
    return all_new_articles

def extract_article_content(url):
    """ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    headers = {"User-Agent": "Mozilla/5.0", "Accept-Language": "ko-KR,ko;q=0.9"}
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        if response.encoding.lower() in ['iso-8859-1', 'euc-kr']:
            response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "lxml")
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'iframe', 'figure']):
            tag.decompose()
        selectors = [
            "#article-view-content-div", "#CmAdContent", "#articleBody", "#article-body", "#view_content_wrap",
            "#article-content-body", "#news_body_area", "#article_content", "#news-contents", "#articleText",
            "article", ".article_body", ".news_end"
        ]
        content_area = next((soup.select_one(s) for s in selectors if soup.select_one(s)), None)
        if content_area:
            text = content_area.get_text(separator='\n', strip=True)
            if len(text) > 100:
                lines = text.split('\n')
                cleaned_lines = []
                for line in lines:
                    if any(marker in line for marker in ARTICLE_END_MARKERS):
                        break
                    cleaned_lines.append(line)
                return '\n'.join(cleaned_lines).strip()
        return "[ì‹¤íŒ¨] ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ ì‹¤íŒ¨"
    except Exception as e:
        return f"[ì˜¤ë¥˜] {str(e)}"

def analyze_articles_with_ai(articles):
    """ê¸°ì‚¬ ëª©ë¡ì„ AIë¥¼ í†µí•´ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    print("\n--- 3ë‹¨ê³„: AI êµ¬ì¡°í™” ë¶„ì„ ì‹œì‘ ---")
    articles_to_process, article_map = [], {}
    for i, article in enumerate(articles):
        article['unique_id'] = f"art_{i}"
        article_map[article['unique_id']] = article
        content_to_analyze = article.get("content", "")
        if not content_to_analyze or content_to_analyze.startswith(("[ì‹¤íŒ¨]", "[ì˜¤ë¥˜]")):
            content_to_analyze = article.get("summary", "")
        if content_to_analyze:
            article['content_to_analyze'] = content_to_analyze
            articles_to_process.append(article)
    print(f"  - AI ë¶„ì„ ëŒ€ìƒ: {len(articles_to_process)}ê°œ / ì´ {len(articles)}ê°œ")
    if not articles_to_process:
        return articles

    for i in range(0, len(articles_to_process), BATCH_SIZE):
        batch = articles_to_process[i:i + BATCH_SIZE]
        print(f"  - ë°°ì¹˜ {i//BATCH_SIZE + 1} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
        batch_content = "\n\n".join([
            f"<article>\n<id>{art['unique_id']}</id>\n<content>\n{art['content_to_analyze']}\n</content>\n</article>"
            for art in batch
        ])
        final_prompt = get_stock_analysis_prompt(batch_content)

        try:
            response = gemini_model.generate_content(final_prompt)
            cleaned_response = response.text.strip().lstrip("```json").lstrip("```").rstrip("```")
            analysis_results_list = json.loads(cleaned_response)
            if isinstance(analysis_results_list, list):
                for result in analysis_results_list:
                    if result.get('id') in article_map:
                        article_map[result['id']].update(result)
            else:
                print(f"    âš ï¸ ë°°ì¹˜ {i//BATCH_SIZE + 1} ë¶„ì„ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜.")
        except Exception as e:
            print(f"    - ë°°ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    final_list = list(article_map.values())
    for art in final_list:
        art.pop('unique_id', None)
        art.pop('id', None)
        art.pop('content_to_analyze', None)
    print("--- âœ… AI ë¶„ì„ ì™„ë£Œ ---")
    return final_list

# ==============================================================================
# ğŸ’¾ 4ë‹¨ê³„: ë°ì´í„° ì·¨í•© ë° CSV ì €ì¥ í•¨ìˆ˜ (JSONBin ëŒ€ì‹  íŒŒì¼ë¡œ ì €ì¥)
# ==============================================================================
def aggregate_and_save_to_csv(new_articles, output_dir):
    """ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ë¡œì»¬ CSV íŒŒì¼ì— ëˆ„ì í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\n--- 4ë‹¨ê³„: ë°ì´í„° ë³‘í•© ë° CSV ì €ì¥ ì‹œì‘ ---")
    if not new_articles:
        print("  - ì·¨í•©í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•­ìƒ ìµœì‹  3ì¼ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë¯€ë¡œ, 
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ì½ì–´ì™€ ë³‘í•©í•˜ëŠ” ëŒ€ì‹  ë§¤ë²ˆ ìƒˆë¡œ ë§Œë“œëŠ” ê²ƒì´ ë” ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì…ë‹ˆë‹¤.
    # (ì–´ì°¨í”¼ ëŒ€ì‹œë³´ë“œëŠ” ìµœì‹  ë°ì´í„°ë§Œ ë³´ì—¬ì¤„ ê²ƒì´ë¯€ë¡œ)
    
    # 1. DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(new_articles)
    
    # 2. ì˜¤ë˜ëœ ë°ì´í„° ì œê±° (ì˜ˆ: ìµœê·¼ 30ì¼ì¹˜ ë°ì´í„°ë§Œ ìœ ì§€)
    thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce').dt.strftime('%Y-%m-%d')
    final_df = df[df['published_at'] >= thirty_days_ago].copy()
    
    # 3. ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì»¬ëŸ¼ì„ CSVì— ì €ì¥í•˜ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
    for col in ['analysis_orgs', 'analysis_keywords']:
        if col in final_df.columns:
            final_df[col] = final_df[col].apply(lambda x: str(x) if isinstance(x, list) else str(x))

    # 4. CSV íŒŒì¼ë¡œ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "aggregated_stock_data.csv")
    final_df.to_csv(csv_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
    print(f"--- âœ… CSV ì €ì¥ ì™„ë£Œ. ì´ {len(final_df)}ê°œ ê¸°ì‚¬ ì €ì¥ ---")
    print(f"   - ì €ì¥ ê²½ë¡œ: {csv_path}")

# ==============================================================================
#  â–¶ï¸ Orchestrator: ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================
def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    print("="*50)
    print(" K-Stock News Analysis Pipeline (GitHub Actions) - START")
    print("="*50)
    
    try:
        initialize_gemini_model()
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

    # ì´ íŒŒì´í”„ë¼ì¸ì€ ë§¤ë²ˆ ìƒˆë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë®ì–´ì“°ë¯€ë¡œ, ê¸°ì¡´ URL ë¡œë“œê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    # ë‹¨, ë„¤ì´ë²„ API ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì‹¤í–‰ ì‹œê°„ ë™ì•ˆì—ëŠ” URLì„ ê¸°ì–µí•©ë‹ˆë‹¤.
    temp_existing_urls = set()
    new_articles = crawl_naver_news(STOCK_SEARCH_KEYWORDS, temp_existing_urls)
    
    if not new_articles:
        print("\nâœ… ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
        
    print("\n--- 2ë‹¨ê³„: ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘ ---")
    for i, article in enumerate(new_articles):
        if not article.get('content'):
            print(f"  - ({i+1}/{len(new_articles)}) ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {article.get('url', '')[:70]}...")
            article['content'] = extract_article_content(article.get('url', ''))
            time.sleep(0.1)
    print("--- âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ---")

    analyzed_articles = analyze_articles_with_ai(new_articles)
    
    # ìµœì¢… ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ê²½ë¡œ ì„¤ì •
    output_dir = os.path.join("output", "aggregated")
    aggregate_and_save_to_csv(analyzed_articles, output_dir)

    print("\n" + "="*50)
    print(" K-Stock News Analysis Pipeline - COMPLETE")
    print("="*50)

if __name__ == "__main__":
    main()
