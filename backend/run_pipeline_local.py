# êµì²´í•  ì½”ë“œ: íŒŒì¼ ìƒë‹¨ import ì˜ì—­
import os
import sys
import json
import time
from datetime import datetime, timedelta
import re
import warnings
import urllib3
import csv
import ast
import collections

# --- í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import requests
import pandas as pd
from bs4 import BeautifulSoup
from dotenv import load_dotenv  # <-- ì¶”ê°€
from tqdm import tqdm          # <-- ì¶”ê°€
import google.generativeai as genai
import google.api_core.exceptions

# --- .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv() # <-- ì¶”ê°€

# --- SSL ê²½ê³  ë¹„í™œì„±í™” ---
warnings.filterwarnings("ignore")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
# ==============================================================================
# ğŸš€ ì„¤ì • ì˜ì—­
# ==============================================================================
# êµì²´í•  ì½”ë“œ: ğŸš€ ì„¤ì • ì˜ì—­
# ==============================================================================
# ğŸš€ ì„¤ì • ì˜ì—­
# ==============================================================================
# êµì²´í•  ì½”ë“œ: ğŸš€ ì„¤ì • ì˜ì—­
# ==============================================================================
# ğŸš€ ì„¤ì • ì˜ì—­
# ==============================================================================

# --- API í‚¤ ë° ID (.env íŒŒì¼ì„ í†µí•´ í™˜ê²½ ë³€ìˆ˜ë¡œ ì „ë‹¬ë°›ìŒ) ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# --- ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ë° ê°œìˆ˜ ì„¤ì • ---
DATA_COLLECTION_DAYS = 1      # ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)
ARTICLES_PER_DAY_LIMIT = 100    # í‚¤ì›Œë“œë‹¹ í•˜ë£¨ì— ìˆ˜ì§‘í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

# --- ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡ ---
STOCK_SEARCH_KEYWORDS = [
    "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "í™˜ìœ¨", "ê¸ˆë¦¬ì¸ìƒ", "FOMC", "ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜", "ë°˜ë„ì²´",
    "HBM", "AIë°˜ë„ì²´", "2ì°¨ì „ì§€", "ë°”ì´ì˜¤", "ì œì•½", "ë°¸ë¥˜ì—…", "ê¸°ì—… ì‹¤ì "
]
# --- ê¸°íƒ€ ì„¤ì • ---
RATE_LIMIT_DELAY = 1
BATCH_SIZE = 7
ARTICLE_END_MARKERS = [
    "ë¬´ë‹¨ì „ì¬", "ë¬´ë‹¨ ì „ì¬", "ì¬ë°°í¬ ê¸ˆì§€", "ì €ì‘ê¶Œì", "ê´‘ê³ ë¬¸ì˜", 
    "ê´‘ê³  ë¬¸ì˜", "ADë§í¬", "íƒ€ë¶ˆë¼", "ê´€ë ¨ê¸°ì‚¬", "ê¸°ìì†Œê°œ", "ê¸°ì ì†Œê°œ",
    "ê¸°ìì˜ ë‹¤ë¥¸ê¸°ì‚¬", "í¸ì§‘íŒ¨ë„", "ë³¸ë¬¸í•˜ë‹¨", "nBYLINE", "ì¢‹ì•„ìš” ë²„íŠ¼",
    "ì†ë³´ëŠ”", "t.me/", "í…”ë ˆê·¸ë¨", "ì˜ìƒì·¨ì¬", "ê¸°ì‚¬ì œë³´", "ë³´ë„ìë£Œ",
    "íŒŸìºìŠ¤íŠ¸", "ë§ì´ ë³¸ ê¸°ì‚¬", "ê³µìœ í•˜ê¸°", "ê³µìœ ë²„íŠ¼", "nCopyright",
    "ê¸°ì‚¬ ì „ì²´ë³´ê¸°", "ì…ë ¥ :", "ì§€ë©´ :", "AIí•™ìŠµ ì´ìš© ê¸ˆì§€", "ê¸°ì‚¬ ê³µìœ ",
    "ëŒ“ê¸€", "ì¢‹ì•„ìš”", "ê´‘ê³ ", "ê´€ë ¨ ë‰´ìŠ¤", "ì¶”ì²œ ë‰´ìŠ¤", "ì˜ìƒí¸ì§‘",
    "ë‰´ìŠ¤ì œê³µ", "ê¸°ì‚¬ì œê³µ", "ê¸°ì‚¬ í•˜ë‹¨ ê´‘ê³ ", "ê¸°ì‚¬ ì˜ì—­ í•˜ë‹¨ ê´‘ê³ ",
    "ê¸°ì ì •ë³´", "ì „ì²´ê¸°ì‚¬ ë³´ê¸°", "ì¥ê¸°ì˜ ê¸°ì", "ê³µê°ì–¸ë¡ ",
    "ê¸°ì (", "ê¸°ì =", "ê¸°ì]", "[ì‚¬ì§„=", "ìë£Œ=", "(ì„œìš¸=ì—°í•©ë‰´ìŠ¤)",
    "[íŒŒì´ë‚¸ì…œë‰´ìŠ¤]", "í˜ì´ìŠ¤ë¶", "íŠ¸ìœ„í„°", "ì¹´ì¹´ì˜¤í†¡", "ì œë³´í•˜ê¸°",
    "ë…ì ì—¬ëŸ¬ë¶„ì˜ ì†Œì¤‘í•œ ì œë³´ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤", "â–¶", "â€»", "â˜", "[â“’", "â—"
]

# ==============================================================================
# ğŸ¤– AI ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜
# ==============================================================================
gemini_model = None

def initialize_gemini_model():
    """Gemini ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global gemini_model
    if not GOOGLE_API_KEY:
        raise ValueError("Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        gemini_model = genai.GenerativeModel("models/gemini-1.5-flash")
        print("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        print(f"ğŸš¨ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def get_stock_analysis_prompt(content):
    """ì£¼ì‹/ê²½ì œ ë‰´ìŠ¤ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return f"""
ë‹¹ì‹ ì€ ìµœê³ ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì— ì œê³µë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬, ê° ê¸°ì‚¬ë³„ë¡œ ì§€ì •ëœ JSON í˜•ì‹ì— ë§ì¶° ì£¼ìš” ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[ë¶„ì„ ê·œì¹™]
- ê° ê¸°ì‚¬ëŠ” `<article>` íƒœê·¸ë¡œ êµ¬ë¶„ë˜ë©°, ê° ê¸°ì‚¬ì˜ `<id>`ë¥¼ JSON ê²°ê³¼ì˜ "id" í•„ë“œì— ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
- `analysis_keywords`: ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë¥¼ 5ê°œ ë‚´ì™¸ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `analysis_orgs`: ê¸°ì‚¬ì— ì–¸ê¸‰ëœ ì£¼ìš” 'ê¸°ê´€/ê¸°ì—…(ORG)'ì„ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤.
- `summary_ai`: ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
- `sentiment_label`: ê¸°ì‚¬ì˜ ì „ë°˜ì ì¸ í†¤ì´ ê¸ì •ì ì¸ì§€(Positive), ë¶€ì •ì ì¸ì§€(Negative), ì¤‘ë¦½ì ì¸ì§€(Neutral) í‰ê°€í•©ë‹ˆë‹¤.
- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë‹¨ì¼ JSON ë¦¬ìŠ¤íŠ¸(ë°°ì—´) í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ì½”ë“œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

[ë¶„ì„í•  ê¸°ì‚¬ ëª©ë¡]
{content}

[ì¶œë ¥ JSON í˜•ì‹]
```json
[
  {{
    "id": "<article>ì˜ id ê°’>",
    "analysis_keywords": ["í‚¤ì›Œë“œ1", ...],
    "analysis_orgs": ["ê¸°ê´€1", ...],
    "summary_ai": "ê¸°ì‚¬ ìš”ì•½",
    "sentiment_label": "Positive, Negative, ë˜ëŠ” Neutral"
  }},
  ...
]
"""
# êµì²´í•  í•¨ìˆ˜: crawl_naver_news (ê¸°ì¡´ í•¨ìˆ˜ë¥¼ í†µì§¸ë¡œ êµì²´)
def crawl_naver_news(keywords, existing_urls):
    """
    ì§€ì •ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•´ ë‚ ì§œë³„ë¡œ ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ ìˆ˜ì§‘í•˜ê³ , ì „ì²´ URL ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    api_url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
    all_new_articles = []
    today = datetime.now()
    
    # 1. ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ ëª©ë¡ ìƒì„±
    target_dates = [(today - timedelta(days=i)).date() for i in range(DATA_COLLECTION_DAYS)]
    target_dates_str = {d.strftime('%Y-%m-%d') for d in target_dates} # ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´ set ì‚¬ìš©
    
    print(f"\n--- 1ë‹¨ê³„: ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ëŒ€ìƒ ê¸°ê°„: ìµœê·¼ {DATA_COLLECTION_DAYS}ì¼, ì¼ë³„ ìµœëŒ€ {ARTICLES_PER_DAY_LIMIT}ê°œ) ---")

    # 2. ê° í‚¤ì›Œë“œì— ëŒ€í•´ ìˆ˜ì§‘ ì‹œì‘
    for keyword in keywords:
        print(f"\n ğŸ” í‚¤ì›Œë“œ '{keyword}' ìˆ˜ì§‘ ì¤‘...")
        
        # í‚¤ì›Œë“œë³„ë¡œ ë‚ ì§œë‹¹ ëª‡ ê°œë¥¼ ìˆ˜ì§‘í–ˆëŠ”ì§€ ì¹´ìš´íŠ¸
        daily_counts = {date_str: 0 for date_str in target_dates_str}
        
        start_index = 1
        keep_searching_for_keyword = True

        # 3. í˜ì´ì§€ë„¤ì´ì…˜ ë£¨í”„ (APIì˜ start ê°’ì„ 1, 101, 201... ìˆœìœ¼ë¡œ ì¦ê°€)
        while keep_searching_for_keyword and start_index <= 1000: # ë„¤ì´ë²„ APIëŠ” ìµœëŒ€ 1000ê°œê¹Œì§€ ì¡°íšŒ ê°€ëŠ¥
            params = {"query": keyword, "display": 100, "start": start_index, "sort": "date"}
            
            try:
                response = requests.get(api_url, headers=headers, params=params, verify=False, timeout=10)
                response.raise_for_status()
                data = response.json()
                items = data.get('items', [])

                if not items:
                    print(f"   - '{keyword}' í‚¤ì›Œë“œì— ëŒ€í•œ ê²°ê³¼ê°€ ë” ì´ìƒ ì—†ìŠµë‹ˆë‹¤.")
                    break # ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì´ í‚¤ì›Œë“œì— ëŒ€í•œ ê²€ìƒ‰ ì¤‘ë‹¨

                found_new_in_batch = False
                for item in items:
                    try:
                        pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S %z').date()
                        pub_date_str = pub_date.strftime('%Y-%m-%d')
                    except (ValueError, TypeError):
                        continue
                        
                    # ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
                    if pub_date_str not in target_dates_str:
                        continue
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ ìˆ˜ì§‘ í•œë„ë¥¼ ì´ˆê³¼í–ˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    if daily_counts[pub_date_str] >= ARTICLES_PER_DAY_LIMIT:
                        continue
                        
                    # URL ì¤‘ë³µ ì²´í¬
                    url = item.get('originallink') or item.get('link')
                    if not url or url in existing_urls:
                        continue
                    
                    # ëª¨ë“  ì¡°ê±´ì„ í†µê³¼í•˜ë©´ ê¸°ì‚¬ ì¶”ê°€
                    title = re.sub('<[^<]+?>', '', item.get('title', ''))
                    summary = re.sub('<[^<]+?>', '', item.get('description', ''))
                    
                    all_new_articles.append({
                        "search_keyword": keyword, "url": url, "title": title, "summary": summary,
                        "crawled_at": datetime.now().isoformat(), "published_at": pub_date_str
                    })
                    
                    existing_urls.add(url) # ì „ì—­ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ URL ì¶”ê°€
                    daily_counts[pub_date_str] += 1 # ì¼ë³„ ì¹´ìš´íŠ¸ ì¦ê°€
                    found_new_in_batch = True

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                start_index += 100
                
                # ëª¨ë“  ë‚ ì§œì— ëŒ€í•´ ìˆ˜ì§‘ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆëŠ”ì§€ ì²´í¬
                if all(count >= ARTICLES_PER_DAY_LIMIT for count in daily_counts.values()):
                    print(f"   - '{keyword}' í‚¤ì›Œë“œì˜ ëª¨ë“  ë‚ ì§œë³„ ìˆ˜ì§‘ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.")
                    keep_searching_for_keyword = False

                time.sleep(RATE_LIMIT_DELAY)

            except Exception as e:
                print(f" âŒ '{keyword}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                break # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ë‹¨

    print(f"\n--- âœ… ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(all_new_articles)}ê°œì˜ ìƒˆ ê¸°ì‚¬ ë°œê²¬ ---")
    return all_new_articles

def extract_article_content(url):
    """ì£¼ì–´ì§„ URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    headers = {"User-Agent": "Mozilla/5.0", "Accept-Language": "ko-KR,ko;q=0.9"}
    try:
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        if response.encoding.lower() in ['iso-8859-1', 'euc-kr']:
            response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "lxml")
        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'iframe', 'figure']):
            tag.decompose()
        selectors = [
            "#article-view-content-div", "#CmAdContent", "#articleBody", "#article-body", "#view_content_wrap",
            "#article-content-body", "#news_body_area", "#article_content", "#news-contents", "#articleText",
            "article", ".article_body", ".news_end"
        ]
        content_area = next((soup.select_one(s) for s in selectors if soup.select_one(s)), None)
        if content_area:
            text = content_area.get_text(separator='\n', strip=True)
            if len(text) > 100:
                lines = text.split('\n')
                cleaned_lines = []
                for line in lines:
                    if any(marker in line for marker in ARTICLE_END_MARKERS):
                        break
                    cleaned_lines.append(line)
                return '\n'.join(cleaned_lines).strip()
        return "[ì‹¤íŒ¨] ë³¸ë¬¸ ì˜ì—­ ì¶”ì¶œ ì‹¤íŒ¨"
    except Exception as e:
        return f"[ì˜¤ë¥˜] {str(e)}"
# ì¶”ê°€í•  í•¨ìˆ˜ 1: ì¤‘ê°„ ë°ì´í„° ì €ì¥
def save_intermediate_data(articles, path="output/intermediate/crawled_data.csv"):
    """ë³¸ë¬¸ ì¶”ì¶œê¹Œì§€ ì™„ë£Œëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not articles:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n--- ğŸ’¾ ì¤‘ê°„ ì €ì¥ ë‹¨ê³„ ---")
    output_dir = os.path.dirname(path)
    os.makedirs(output_dir, exist_ok=True)
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    df = pd.DataFrame(articles)
    df.to_csv(path, index=False, encoding='utf-8-sig')
    print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œëœ ê¸°ì‚¬ {len(df)}ê°œë¥¼ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {path}")
    return path

# ì¶”ê°€í•  í•¨ìˆ˜ 2: ì¤‘ê°„ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def load_intermediate_data(path="output/intermediate/crawled_data.csv"):
    """íŒŒì¼ë¡œ ì €ì¥ëœ ì¤‘ê°„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if not os.path.exists(path):
        return None
        
    print(f"\n--- ğŸ’¾ ì¤‘ê°„ ë°ì´í„° ë¡œë”© ---")
    print(f"âœ… ì €ì¥ëœ ì¤‘ê°„ ë°ì´í„° íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: {path}")
    print("ìˆ˜ì§‘ ë° ë³¸ë¬¸ ì¶”ì¶œ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ì´ íŒŒì¼ì—ì„œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    df = pd.read_csv(path)
    # CSVë¡œ ì €ì¥ë˜ë©´ì„œ ë¬¸ìì—´ì´ ëœ ì»¬ëŸ¼ë“¤ì„ ë‹¤ì‹œ ì›ë˜ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    # ì´ ì˜ˆì œì—ì„œëŠ” ëª¨ë“  ì»¬ëŸ¼ì´ ë¬¸ìì—´ì´ë¯€ë¡œ, DataFrameì„ ë°”ë¡œ ë¦¬ìŠ¤íŠ¸(dict)ë¡œ ë³€í™˜
    articles = df.to_dict('records')
    return articles

def analyze_articles_with_ai(articles):
    """ê¸°ì‚¬ ëª©ë¡ì„ AIë¥¼ í†µí•´ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    print("\n--- 3ë‹¨ê³„: AI êµ¬ì¡°í™” ë¶„ì„ ì‹œì‘ ---")
    articles_to_process, article_map = [], {}
    for i, article in enumerate(articles):
        article['unique_id'] = f"art_{i}"
        article_map[article['unique_id']] = article
        content_to_analyze = article.get("content", "")
        if not content_to_analyze or content_to_analyze.startswith(("[ì‹¤íŒ¨]", "[ì˜¤ë¥˜]")):
            content_to_analyze = article.get("summary", "")
        if content_to_analyze:
            article['content_to_analyze'] = content_to_analyze
            articles_to_process.append(article)
    print(f"  - AI ë¶„ì„ ëŒ€ìƒ: {len(articles_to_process)}ê°œ / ì´ {len(articles)}ê°œ")
    if not articles_to_process:
        return articles

    for i in range(0, len(articles_to_process), BATCH_SIZE):
        batch = articles_to_process[i:i + BATCH_SIZE]
        print(f"  - ë°°ì¹˜ {i//BATCH_SIZE + 1} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
        batch_content = "\n\n".join([
            f"<article>\n<id>{art['unique_id']}</id>\n<content>\n{art['content_to_analyze']}\n</content>\n</article>"
            for art in batch
        ])
        final_prompt = get_stock_analysis_prompt(batch_content)

        try:
            response = gemini_model.generate_content(final_prompt)
            cleaned_response = response.text.strip().lstrip("```json").lstrip("```").rstrip("```")
            analysis_results_list = json.loads(cleaned_response)
            if isinstance(analysis_results_list, list):
                for result in analysis_results_list:
                    if result.get('id') in article_map:
                        article_map[result['id']].update(result)
            else:
                print(f"    âš ï¸ ë°°ì¹˜ {i//BATCH_SIZE + 1} ë¶„ì„ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜.")
        except Exception as e:
            print(f"    - ë°°ì¹˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    final_list = list(article_map.values())
    for art in final_list:
        art.pop('unique_id', None)
        art.pop('id', None)
        art.pop('content_to_analyze', None)
    print("--- âœ… AI ë¶„ì„ ì™„ë£Œ ---")
    return final_list

# ==============================================================================
# ğŸ’¾ 4ë‹¨ê³„: ë°ì´í„° ì·¨í•© ë° CSV ì €ì¥ í•¨ìˆ˜ (JSONBin ëŒ€ì‹  íŒŒì¼ë¡œ ì €ì¥)
# ==============================================================================
def aggregate_and_save_to_csv(new_articles, output_dir):
    """ìƒˆë¡œìš´ ê¸°ì‚¬ë¥¼ ë¡œì»¬ CSV íŒŒì¼ì— ëˆ„ì í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\n--- 4ë‹¨ê³„: ë°ì´í„° ë³‘í•© ë° CSV ì €ì¥ ì‹œì‘ ---")
    if not new_articles:
        print("  - ì·¨í•©í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•­ìƒ ìµœì‹  3ì¼ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ë¯€ë¡œ, 
    # ê¸°ì¡´ ë°ì´í„°ë¥¼ ì½ì–´ì™€ ë³‘í•©í•˜ëŠ” ëŒ€ì‹  ë§¤ë²ˆ ìƒˆë¡œ ë§Œë“œëŠ” ê²ƒì´ ë” ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì…ë‹ˆë‹¤.
    # (ì–´ì°¨í”¼ ëŒ€ì‹œë³´ë“œëŠ” ìµœì‹  ë°ì´í„°ë§Œ ë³´ì—¬ì¤„ ê²ƒì´ë¯€ë¡œ)
    
    # 1. DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(new_articles)
    
    # 2. ì˜¤ë˜ëœ ë°ì´í„° ì œê±° (ì˜ˆ: ìµœê·¼ 30ì¼ì¹˜ ë°ì´í„°ë§Œ ìœ ì§€)
    thirty_days_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce').dt.strftime('%Y-%m-%d')
    final_df = df[df['published_at'] >= thirty_days_ago].copy()
    
    # 3. ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì»¬ëŸ¼ì„ CSVì— ì €ì¥í•˜ê¸° ì¢‹ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
    for col in ['analysis_orgs', 'analysis_keywords']:
        if col in final_df.columns:
            final_df[col] = final_df[col].apply(lambda x: str(x) if isinstance(x, list) else str(x))

    # 4. CSV íŒŒì¼ë¡œ ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    csv_path = os.path.join(output_dir, "aggregated_stock_data.csv")
    final_df.to_csv(csv_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)
    print(f"--- âœ… CSV ì €ì¥ ì™„ë£Œ. ì´ {len(final_df)}ê°œ ê¸°ì‚¬ ì €ì¥ ---")
    print(f"   - ì €ì¥ ê²½ë¡œ: {csv_path}")

# êµì²´í•  í•¨ìˆ˜: main (ê¸°ì¡´ í•¨ìˆ˜ë¥¼ í†µì§¸ë¡œ êµì²´)
def main():
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
    """
    print("="*60)
    print(" K-Stock News Analysis Pipeline (Local, Resumable) - START")
    print("="*60)

    # --- [í•µì‹¬ ìˆ˜ì •] ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ì‹¤ì œ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì • ---
    # 1. ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    #    (ì˜ˆ: P:\stock_crawl\backend)
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

    # 2. ì´ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ê°„ ë° ìµœì¢… ì €ì¥ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    #    (ì˜ˆ: P:\stock_crawl\backend\output\intermediate\crawled_data.csv)
    intermediate_file_path = os.path.join(SCRIPT_DIR, "output", "intermediate", "crawled_data.csv")
    final_output_dir = os.path.join(SCRIPT_DIR, "output", "aggregated")
    # -----------------------------------------------------------------

    # ì¤‘ê°„ ë°ì´í„° íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
    analyzed_articles = None
    articles_to_process = load_intermediate_data(intermediate_file_path)

    # ì¤‘ê°„ íŒŒì¼ì´ ì—†ìœ¼ë©´, ìˆ˜ì§‘ë¶€í„° ì‹œì‘
    if articles_to_process is None:
        print("\nì¤‘ê°„ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ìˆ˜ì§‘ë¶€í„° ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        try:
            initialize_gemini_model()
        except Exception as e:
            print(f"âŒ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            return

        temp_existing_urls = set()
        new_articles = crawl_naver_news(STOCK_SEARCH_KEYWORDS, temp_existing_urls)
        
        if not new_articles:
            print("\nâœ… ìˆ˜ì§‘ëœ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
            
        print("\n--- 2ë‹¨ê³„: ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘ ---")
        for article in tqdm(new_articles, desc="  - ë³¸ë¬¸ ì¶”ì¶œ ì¤‘"):
            if not article.get('content'):
                article['content'] = extract_article_content(article.get('url', ''))
                time.sleep(0.1)
        print("--- âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ---")

        # ë³¸ë¬¸ ì¶”ì¶œ í›„, AI ë¶„ì„ ì „ì— ì¤‘ê°„ íŒŒì¼ë¡œ ì €ì¥
        save_intermediate_data(new_articles, intermediate_file_path)
        articles_to_process = new_articles
    
    # AI ë¶„ì„ ì‹¤í–‰ (ìƒˆë¡œ ìˆ˜ì§‘í–ˆê±°ë‚˜, íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì™”ê±°ë‚˜)
    if articles_to_process:
        try:
            if 'gemini_model' not in globals() or gemini_model is None:
                initialize_gemini_model()
            
            analyzed_articles = analyze_articles_with_ai(articles_to_process)
            
            aggregate_and_save_to_csv(analyzed_articles, final_output_dir)

            if os.path.exists(intermediate_file_path):
                os.remove(intermediate_file_path)
                print(f"\nâœ… ìµœì¢… ë¶„ì„ ì™„ë£Œ. ì¤‘ê°„ íŒŒì¼({intermediate_file_path})ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print("\n" + "="*60)
            print(f"ğŸš¨ AI ë¶„ì„ ë˜ëŠ” ìµœì¢… ì €ì¥ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            print(f"ğŸ‘ í•˜ì§€ë§Œ ê±±ì •ë§ˆì„¸ìš”! ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” '{intermediate_file_path}'ì— ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            print("   ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì €ì¥ëœ ë°ì´í„°ë¡œ AI ë¶„ì„ì„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            print("="*60)
            return

    print("\n" + "="*60)
    print(" K-Stock News Analysis Pipeline - COMPLETE")
    print("="*60)
# êµì²´í•  ì½”ë“œ: if __name__ == "__main__": ë¸”ë¡ ì „ì²´
if __name__ == "__main__":
    main()

    # (ì¶”ê°€) í‚¤ì›Œë“œ ì§‘ê³„ ë° ì¶”ì²œ
    print("\n[ì¶”ì²œ í‚¤ì›Œë“œ ë¶„ì„]")
    try:
        latest_path = os.path.join("output", "aggregated", "aggregated_stock_data.csv")
        if not os.path.exists(latest_path):
             print("ë¶„ì„í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            df = pd.read_csv(latest_path, encoding='utf-8')
            df.dropna(subset=['analysis_keywords'], inplace=True) # í‚¤ì›Œë“œê°€ ì—†ëŠ” í–‰ ì œê±°

            all_keywords = []
            # [ìˆ˜ì •] ast.literal_eval ì˜¤ë¥˜ ë°œìƒ ì‹œ ê±´ë„ˆë›°ë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
            for x in df['analysis_keywords']:
                try:
                    kws = ast.literal_eval(str(x))
                    if isinstance(kws, list):
                        all_keywords.extend([k for k in kws if isinstance(k, str)])
                except (ValueError, SyntaxError):
                    continue

            keyword_counts = collections.Counter(all_keywords)
            recommended_keywords = [
                (kw, count)
                for kw, count in keyword_counts.most_common(50)
                if kw not in STOCK_SEARCH_KEYWORDS and len(kw) > 1
            ][:20]

            # [ìˆ˜ì •] DATA_COLLECTION_DAYS ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ì¶œë ¥
            print(f"ğŸ’¡ ìµœê·¼ {DATA_COLLECTION_DAYS}ì¼ê°„ ì¶”ì²œ ê²€ìƒ‰ í‚¤ì›Œë“œ:")
            for kw, count in recommended_keywords:
                print(f"- {kw} ({count}íšŒ)")

    except Exception as e:
        print(f"[í‚¤ì›Œë“œ ì¶”ì²œ ë¶„ì„ ì˜¤ë¥˜] {e}")