import pandas as pd
import ast
import collections

# ê¸°ì¡´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
STOCK_SEARCH_KEYWORDS = ["ì½”ìŠ¤í”¼", "ì—°ì¤€", "ì‚¼ì„±ì „ì", "2ë¶„ê¸° ì‹¤ì ", 
"ì£¼ê°€ ìƒìŠ¹", "íŒŒì›” ì˜ì¥", "ë°˜ë„ì²´", "ê¸ˆë¦¬ ë™ê²°", "í•œë¯¸ ê´€ì„¸ í˜‘ìƒ", "ì½”ìŠ¤ë‹¥"]

# ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ
CSV_PATH = r"P:\stock_crawl\backend\output\merged_no_duplicate.csv"
df = pd.read_csv(CSV_PATH, encoding="utf-8")
df['published_at'] = pd.to_datetime(df['published_at'])
date_limit = pd.Timestamp.now() - pd.Timedelta(days=30)
df = df[df['published_at'] >= date_limit]

# í‚¤ì›Œë“œ ì „ì²´ ì§‘ê³„
all_keywords = []
for x in df['analysis_keywords']:
    try:
        kws = ast.literal_eval(str(x))
        all_keywords.extend([k for k in kws if isinstance(k, str)])
    except Exception:
        continue
keyword_counts = collections.Counter(all_keywords)

# ê¸°ì¡´ í‚¤ì›Œë“œë³„ ë“±ì¥ ë¹ˆë„
print("\n[ê¸°ì¡´ í‚¤ì›Œë“œë³„ ìµœê·¼ 30ì¼ ê¸°ì‚¬ ë“±ì¥ìˆ˜]")
for kw in STOCK_SEARCH_KEYWORDS:
    print(f"{kw}: {keyword_counts[kw]}íšŒ")

# â€œì¤‘ìš”ë„ ë‚®ì€ í‚¤ì›Œë“œâ€ ìë™ ì •ë¦¬(3íšŒ ì´í•˜ ë“± ì„ê³„ê°’ ì ìš©)
LOW_IMPORTANCE = [kw for kw in STOCK_SEARCH_KEYWORDS if keyword_counts[kw] <= 3]
print("\nğŸ’¡ ìµœê·¼ 30ì¼ê°„ ê±°ì˜ ë“±ì¥í•˜ì§€ ì•Šì€ 'ì •ë¦¬ ì¶”ì²œ' í‚¤ì›Œë“œ:")
for kw in LOW_IMPORTANCE:
    print(f"- {kw}")

# "ìƒìœ„ Nê°œë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ìë™ ì œê±°" ì˜ˆì‹œ (N=10)
N = 10
top_keywords = [kw for kw, _ in keyword_counts.most_common(N)]
print(f"\nğŸ’¡ ì¶”ì²œ ìƒìœ„ {N}ê°œ í‚¤ì›Œë“œë§Œ ë‚¨ê¸°ê¸°:", top_keywords)
